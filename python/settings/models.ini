[gender]
input_shape: (100,100,3)
nb_classes: 2
model_path: '../models/gender/final/gender_1.h5'
weights_path: '../models/gender/final/gender_1_weights.h5'
check_path: '../models/gender/'
train_path: '../data/Adience/gender/train/'
eval_path: '../data/Adience/gender/eval/'
early_stop_th: 0.000000001
learning_rate: 0.01
momentum: 0.95
batch_size: 16
epochs: 50
steps_per_epoch: 1000
validation_steps: 800
target_size: (100,100)
log_path: '../logs/gender_log.csv'
label_csv: '../data/Adience/labels.csv'
h5_input: '../data/Adience/hdf5/adience-100.h5'


[faces12net]
input_shape: (12,12,3)
nb_classes: 2
model_path: '../models/faces12net.h5'
check_path: '../models/faces12net/weights.{epoch:02d}-{val_loss:.2f}.h5'
train_path: '../data/facedetection/train/'
eval_path: '../data/facedetection/eval/'
early_stop_th: 0.000000001
learning_rate: 0.01
momentum: 0.95
batch_size: 64
epochs: 1000
steps_per_epoch: 500
validation_steps: 10
target_size: (12,12)
log_path: '../logs/face12net_log.csv'


[vgg]
input_shape: (200,200,3)
nb_classes: 2
model_path: '../models/facedetection/vgg/final/vgg.h5'
weights_path: '../models/facedetection/vgg/final/vgg_weights.h5'
check_path: '../models/facedetection/vgg/weights.{epoch:02d}-{val_loss:.2f}.h5'
train_path: '../data/facedetection/train/'
eval_path: '../data/facedetection/eval/'
early_stop_th: 0.000000001
learning_rate: 0.01
momentum: 0.95
batch_size: 64
epochs: 50
steps_per_epoch: 500
validation_steps: 10
target_size: (200,200)
log_path: '../logs/vgg.csv'
label_csv: '../data/facedetection/vgg/final.csv'
h5_input: '../data/facedetection/vgg/vgg.h5'


