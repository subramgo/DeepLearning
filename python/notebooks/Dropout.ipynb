{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout : A simple way to Prevent Neural Networks from Overfitting\n",
    "\n",
    "\n",
    "In this notebook, we experiment with dropouts on MNIST data set. The aim is recreate the experiment results given in section 6.1.1 in the paper https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "This section compares the neural networks with and without dropouts for MNIST dataset.\n",
    "\n",
    "MNIST dataset is available with Keras. It consists of 28 x 28 pixel handwritten digit images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape (60000, 28, 28)\n",
      "y_train shape (60000,)\n",
      "x_test shape (10000, 28, 28)\n",
      "x_train shape (10000,)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(\"x_train shape {}\".format(x_train.shape))\n",
    "print(\"y_train shape {}\".format(y_train.shape))\n",
    "print(\"x_test shape {}\".format(x_test.shape))\n",
    "print(\"x_train shape {}\".format(y_test.shape))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAT4AAAD8CAYAAADub8g7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nP8ELwPtu9XkdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11f319fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(x_train[0], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(x_train[10], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(x_train[500], cmap = plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(x_train[1020], cmap = plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train one hot shapec (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "## One hot encoder\n",
    "train_one_hot =  np_utils.to_categorical(y_train)\n",
    "test_one_hot  = np_utils.to_categorical(y_test)\n",
    "\n",
    "print(\"train one hot shapec {}\".format(train_one_hot.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = x_train.shape[1:]\n",
    "input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 \n",
    "----\n",
    "28 x 28 --- Flatten --> 784 --Dense--> 1024, Relu --Dense--> 1024, Relu --softmax--> 10 \n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "momentum = 0.0\n",
    "decay = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, Flatten\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "from keras.models import Model \n",
    "from keras import optimizers\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "momentum = 0.0\n",
    "decay = 0.0\n",
    "early_stop_th = 10**-5\n",
    "\n",
    "\n",
    "# Stop the training if the accuracy is not moving more than a delta\n",
    "# keras.callbacks.History is by default added to all keras model\n",
    "callbacks = [EarlyStopping(monitor='acc', min_delta=early_stop_th, patience=5, verbose=0, mode='auto')]\n",
    "\n",
    "# Code up the network\n",
    "x_input = Input(input_dim)\n",
    "x = Flatten()(x_input)\n",
    "x = Dense(1024, activation='relu', name =\"dense1\")(x)\n",
    "x = Dense(1024, activation='relu', name = \"dense2\")(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Optimizer\n",
    "sgd = optimizers.SGD(lr=0.01, momentum=0, decay=0, nesterov=False)\n",
    "\n",
    "\n",
    "# Create and train model\n",
    "model = Model(inputs = x_input, outputs = predictions)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "hist = model.fit(x_train, train_one_hot, validation_split = 0.1, batch_size = batch_size,callbacks = callbacks ,epochs = epochs, verbose = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training(hist):\n",
    "\n",
    "    fig = plt.figure(figsize=(8,6))\n",
    "    plt.tight_layout()\n",
    "    plt.subplot(121)\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title(\"Train Loss and Validation Loss\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.subplot(122)\n",
    "    plt.plot(hist.history['acc'])\n",
    "    plt.plot(hist.history['val_acc'])\n",
    "    plt.title(\"Train Accuracy and Validation Accurcy\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"accuracy\")\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "\n",
    "\n",
    "plot_training(hist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model on train and test\n",
    "\n",
    "def print_metrics(model,x_train, train_one_hot, x_test, test_one_hot):\n",
    "    metrics = model.evaluate(x=x_train, y=train_one_hot, batch_size=batch_size, verbose=1, sample_weight=None, steps=None)\n",
    "    print(\"Train Evaluation\\n\")\n",
    "    for k,v in zip(model.metrics_names,metrics):\n",
    "        print (k,v)\n",
    "    print('error {}'.format(1-v))\n",
    "\n",
    "    print(\"\\nTest Evaluation\\n\")\n",
    "    metrics = model.evaluate(x=x_test, y=test_one_hot, batch_size=batch_size, verbose=1, sample_weight=None, steps=None)\n",
    "    for k,v in zip(model.metrics_names,metrics):\n",
    "        print (k,v)\n",
    "    print('error {}'.format(1-v))\n",
    "\n",
    "\n",
    "print_metrics(model,x_train, train_one_hot, x_test, test_one_hot)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refer to ../scripts/folder for experiments\n",
    "\n",
    "Final hyper paramters from the experiments\n",
    "\n",
    "Best performing model chosen hyper-parameters:\n",
    "{'Dropout_1': 0.026079803111884514, 'Dropout': 0.4844455237320119}\n",
    "\n",
    "\n",
    "Best performing model chosen hyper-parameters:\n",
    "{'lr': 0.8713270582626444}\n",
    "\n",
    "\n",
    "{'momentum': 0.8671876498073315}\n",
    "\n",
    "\n",
    "Best performing model chosen hyper-parameters:\n",
    "{'max_norm': 2.886226647301249, 'max_norm_1': 1.0069271927587264}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 54000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "54000/54000 [==============================] - 10s 177us/step - loss: 0.6935 - acc: 0.8012 - val_loss: 0.2437 - val_acc: 0.9300\n",
      "Epoch 2/50\n",
      "54000/54000 [==============================] - 9s 169us/step - loss: 0.5075 - acc: 0.8505 - val_loss: 0.2676 - val_acc: 0.9308\n",
      "Epoch 3/50\n",
      "54000/54000 [==============================] - 9s 172us/step - loss: 0.4996 - acc: 0.8563 - val_loss: 0.3459 - val_acc: 0.8872\n",
      "Epoch 4/50\n",
      "54000/54000 [==============================] - 9s 170us/step - loss: 0.5081 - acc: 0.8537 - val_loss: 0.3486 - val_acc: 0.8992\n",
      "Epoch 5/50\n",
      "54000/54000 [==============================] - 10s 193us/step - loss: 0.6460 - acc: 0.8081 - val_loss: 0.4260 - val_acc: 0.8907\n",
      "Epoch 6/50\n",
      "54000/54000 [==============================] - 9s 176us/step - loss: 0.6477 - acc: 0.8062 - val_loss: 0.4814 - val_acc: 0.8610\n",
      "Epoch 7/50\n",
      "54000/54000 [==============================] - 9s 175us/step - loss: 0.6965 - acc: 0.7941 - val_loss: 0.5539 - val_acc: 0.8170\n",
      "Epoch 8/50\n",
      "54000/54000 [==============================] - 9s 173us/step - loss: 0.7925 - acc: 0.7636 - val_loss: 0.5750 - val_acc: 0.8612\n",
      "Accuracy 0.8367  Error 0.1633\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Dropout\n",
    "from keras.callbacks import History, EarlyStopping\n",
    "from keras.models import Model \n",
    "from keras import optimizers\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from hyperas.distributions import uniform\n",
    "from hyperas import optim\n",
    "from hyperopt import Trials, STATUS_OK, tpe\n",
    "from keras.constraints import max_norm\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "\n",
    "\n",
    "def mnist_data():\n",
    "    \"\"\"\n",
    "    Data providing function:\n",
    "\n",
    "    This function is separated from create_model() so that hyperopt\n",
    "    won't reload data for each evaluation run.\n",
    "    \"\"\"\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(60000, 784)\n",
    "    x_test = x_test.reshape(10000, 784)\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    nb_classes = 10\n",
    "    y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "    return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "x_train, y_train, x_test, y_test = mnist_data()\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 50\n",
    "learning_rate = 0.8713270582626444\n",
    "momentum = 0.8671876498073315\n",
    "decay = 0.0\n",
    "early_stop_th = 10**-10\n",
    "input_dim = (784,)\n",
    "\n",
    "dropout_1 = 0.026079803111884514\n",
    "dropout_2 = 0.4844455237320119\n",
    "\n",
    "mx_n1 = 2.88622664730124\n",
    "mx_n2 = 1.0069271927587264\n",
    "\n",
    "x_input = Input(input_dim)\n",
    "x = Dropout(dropout_1)(x_input)\n",
    "x = Dense(1024, activation='relu', name =\"dense1\",kernel_constraint=max_norm( mx_n1 ) )(x)\n",
    "x = Dropout(dropout_2)(x)\n",
    "x = Dense(1024, activation='relu', name = \"dense2\",kernel_constraint=max_norm( mx_n2 ) )(x)\n",
    "predictions = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Optimizer\n",
    "sgd = optimizers.SGD(lr=learning_rate, momentum=momentum, decay=0, nesterov=False)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='acc', min_delta=early_stop_th, patience=5, verbose=0, mode='auto')]\n",
    "\n",
    "\n",
    "# Create and train model\n",
    "model = Model(inputs = x_input, outputs = predictions)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x=x_train,y= y_train, validation_split = 0.1, callbacks = callbacks, batch_size = batch_size ,epochs = epochs, verbose = 1)\n",
    "metrics = model.evaluate(x=x_test, y=y_test, batch_size=batch_size, verbose=0, sample_weight=None, steps=None)\n",
    "\n",
    "\n",
    "accuracy = metrics[1]\n",
    "\n",
    "print(\"Accuracy {}  Error {}\".format(accuracy, 1-accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
